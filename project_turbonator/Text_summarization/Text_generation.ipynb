{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credit: https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ascii text and covert to lowercase\n",
    "filename = \"../../../dataset/Text_data/wonderland.txt\"\n",
    "raw_text = open(filename).read()\n",
    "raw_text = raw_text.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  144409\n",
      "Total Vocab:  45\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print (\"Total Characters: \", n_chars)\n",
    "print (\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  144309\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "\tseq_in = raw_text[i:i + seq_length]\n",
    "\tseq_out = raw_text[i + seq_length]\n",
    "\tdataX.append([char_to_int[char] for char in seq_in])\n",
    "\tdataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print (\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.epochs_since_last_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "144309/144309 [==============================] - 155s 1ms/step - loss: 2.9725\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.97250, saving model to weights-improvement-01-2.9725.hdf5\n",
      "Epoch 2/20\n",
      "144309/144309 [==============================] - 154s 1ms/step - loss: 2.7677\n",
      "\n",
      "Epoch 00002: loss improved from 2.97250 to 2.76775, saving model to weights-improvement-02-2.7677.hdf5\n",
      "Epoch 3/20\n",
      "144309/144309 [==============================] - 154s 1ms/step - loss: 2.6647\n",
      "\n",
      "Epoch 00003: loss improved from 2.76775 to 2.66470, saving model to weights-improvement-03-2.6647.hdf5\n",
      "Epoch 4/20\n",
      "144309/144309 [==============================] - 155s 1ms/step - loss: 2.5885\n",
      "\n",
      "Epoch 00004: loss improved from 2.66470 to 2.58851, saving model to weights-improvement-04-2.5885.hdf5\n",
      "Epoch 5/20\n",
      "144309/144309 [==============================] - 155s 1ms/step - loss: 2.5238\n",
      "\n",
      "Epoch 00005: loss improved from 2.58851 to 2.52384, saving model to weights-improvement-05-2.5238.hdf5\n",
      "Epoch 6/20\n",
      "144309/144309 [==============================] - 154s 1ms/step - loss: 2.4641\n",
      "\n",
      "Epoch 00006: loss improved from 2.52384 to 2.46413, saving model to weights-improvement-06-2.4641.hdf5\n",
      "Epoch 7/20\n",
      "144309/144309 [==============================] - 154s 1ms/step - loss: 2.4102\n",
      "\n",
      "Epoch 00007: loss improved from 2.46413 to 2.41022, saving model to weights-improvement-07-2.4102.hdf5\n",
      "Epoch 8/20\n",
      "144309/144309 [==============================] - 155s 1ms/step - loss: 2.3622\n",
      "\n",
      "Epoch 00008: loss improved from 2.41022 to 2.36219, saving model to weights-improvement-08-2.3622.hdf5\n",
      "Epoch 9/20\n",
      "144309/144309 [==============================] - 155s 1ms/step - loss: 2.3135\n",
      "\n",
      "Epoch 00009: loss improved from 2.36219 to 2.31352, saving model to weights-improvement-09-2.3135.hdf5\n",
      "Epoch 10/20\n",
      "144309/144309 [==============================] - 155s 1ms/step - loss: 2.2667\n",
      "\n",
      "Epoch 00010: loss improved from 2.31352 to 2.26671, saving model to weights-improvement-10-2.2667.hdf5\n",
      "Epoch 11/20\n",
      "144309/144309 [==============================] - 155s 1ms/step - loss: 2.2256\n",
      "\n",
      "Epoch 00011: loss improved from 2.26671 to 2.22559, saving model to weights-improvement-11-2.2256.hdf5\n",
      "Epoch 12/20\n",
      "144309/144309 [==============================] - 155s 1ms/step - loss: 2.1848\n",
      "\n",
      "Epoch 00012: loss improved from 2.22559 to 2.18483, saving model to weights-improvement-12-2.1848.hdf5\n",
      "Epoch 13/20\n",
      "144309/144309 [==============================] - 155s 1ms/step - loss: 2.1433\n",
      "\n",
      "Epoch 00013: loss improved from 2.18483 to 2.14328, saving model to weights-improvement-13-2.1433.hdf5\n",
      "Epoch 14/20\n",
      "144309/144309 [==============================] - 154s 1ms/step - loss: 2.1044\n",
      "\n",
      "Epoch 00014: loss improved from 2.14328 to 2.10436, saving model to weights-improvement-14-2.1044.hdf5\n",
      "Epoch 15/20\n",
      "144309/144309 [==============================] - 155s 1ms/step - loss: 2.0666\n",
      "\n",
      "Epoch 00015: loss improved from 2.10436 to 2.06656, saving model to weights-improvement-15-2.0666.hdf5\n",
      "Epoch 16/20\n",
      "144309/144309 [==============================] - 155s 1ms/step - loss: 2.0332\n",
      "\n",
      "Epoch 00016: loss improved from 2.06656 to 2.03324, saving model to weights-improvement-16-2.0332.hdf5\n",
      "Epoch 17/20\n",
      "144309/144309 [==============================] - 155s 1ms/step - loss: 2.0180\n",
      "\n",
      "Epoch 00017: loss improved from 2.03324 to 2.01800, saving model to weights-improvement-17-2.0180.hdf5\n",
      "Epoch 18/20\n",
      "144309/144309 [==============================] - 155s 1ms/step - loss: 1.9599\n",
      "\n",
      "Epoch 00018: loss improved from 2.01800 to 1.95986, saving model to weights-improvement-18-1.9599.hdf5\n",
      "Epoch 19/20\n",
      "144309/144309 [==============================] - 155s 1ms/step - loss: 1.9371\n",
      "\n",
      "Epoch 00019: loss improved from 1.95986 to 1.93714, saving model to weights-improvement-19-1.9371.hdf5\n",
      "Epoch 20/20\n",
      "144309/144309 [==============================] - 155s 1ms/step - loss: 1.9223\n",
      "\n",
      "Epoch 00020: loss improved from 1.93714 to 1.92229, saving model to weights-improvement-20-1.9223.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28bb7d91cc0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the network weights\n",
    "filename = \"weights-improvement-20-1.9223.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\"  she had gone through that day.\n",
      "\n",
      "'a likely story indeed!' said the pigeon in a tone of the deepest\n",
      "c \"\n",
      "areen. ''d'ded tou wail to toe to teln to toal ' she said to herself, 'and that so see th the tou oo aalit it '\n",
      "\n",
      "'i sh soe the sagt tfree tfree tarter,' said the daterpillar.\n",
      "\n",
      "'ien a cen,' said the daterpillar.\n",
      "\n",
      "'ien a cen,' said the daterpillar.\n",
      "\n",
      "'ien a cen,' said the daterpillar.\n",
      "\n",
      "'ien a cen,' said the daterpillar.\n",
      "\n",
      "'ien a cen,' said the daterpillar.\n",
      "\n",
      "'ien a cen,' said the daterpillar.\n",
      "\n",
      "'ien a cen,' said the daterpillar.\n",
      "\n",
      "'ien a cen,' said the daterpillar.\n",
      "\n",
      "'ien a cen,' said the daterpillar.\n",
      "\n",
      "'ien a cen,' said the daterpillar.\n",
      "\n",
      "'ien a cen,' said the daterpillar.\n",
      "\n",
      "'ien a cen,' said the daterpillar.\n",
      "\n",
      "'ien a cen,' said the daterpillar.\n",
      "\n",
      "'ien a cen,' said the daterpillar.\n",
      "\n",
      "'ien a cen,' said the daterpillar.\n",
      "\n",
      "'ien a cen,' said the daterpillar.\n",
      "\n",
      "'ien a cen,' said the daterpillar.\n",
      "\n",
      "'ien a cen,' said the daterpillar.\n",
      "\n",
      "'ien a cen,' said the daterpillar.\n",
      "\n",
      "'ien a cen,' said the daterpillar.\n",
      "\n",
      "'ien a cen,' said the daterpillar.\n",
      "\n",
      "'ien a cen,' said the daterpillar.\n",
      "\n",
      "'ien a cen,' said the daterpilla\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print (\"Seed:\")\n",
    "print (\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "\tx = x / float(n_vocab)\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = numpy.argmax(prediction)\n",
    "\tresult = int_to_char[index]\n",
    "\tseq_in = [int_to_char[value] for value in pattern]\n",
    "\tsys.stdout.write(result)\n",
    "\tpattern.append(index)\n",
    "\tpattern = pattern[1:len(pattern)]\n",
    "print (\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "144309/144309 [==============================] - 627s 4ms/step - loss: 2.7750\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.77500, saving model to weights-improvement-01-2.7750-bigger.hdf5\n",
      "Epoch 2/50\n",
      "144309/144309 [==============================] - 631s 4ms/step - loss: 2.4106\n",
      "\n",
      "Epoch 00002: loss improved from 2.77500 to 2.41058, saving model to weights-improvement-02-2.4106-bigger.hdf5\n",
      "Epoch 3/50\n",
      "144309/144309 [==============================] - 627s 4ms/step - loss: 2.2042\n",
      "\n",
      "Epoch 00003: loss improved from 2.41058 to 2.20421, saving model to weights-improvement-03-2.2042-bigger.hdf5\n",
      "Epoch 4/50\n",
      "144309/144309 [==============================] - 625s 4ms/step - loss: 2.0739\n",
      "\n",
      "Epoch 00004: loss improved from 2.20421 to 2.07395, saving model to weights-improvement-04-2.0739-bigger.hdf5\n",
      "Epoch 5/50\n",
      "144309/144309 [==============================] - 625s 4ms/step - loss: 1.9765\n",
      "\n",
      "Epoch 00005: loss improved from 2.07395 to 1.97651, saving model to weights-improvement-05-1.9765-bigger.hdf5\n",
      "Epoch 6/50\n",
      "144309/144309 [==============================] - 625s 4ms/step - loss: 1.9028\n",
      "\n",
      "Epoch 00006: loss improved from 1.97651 to 1.90283, saving model to weights-improvement-06-1.9028-bigger.hdf5\n",
      "Epoch 7/50\n",
      "144309/144309 [==============================] - 626s 4ms/step - loss: 1.8414\n",
      "\n",
      "Epoch 00007: loss improved from 1.90283 to 1.84139, saving model to weights-improvement-07-1.8414-bigger.hdf5\n",
      "Epoch 8/50\n",
      "144309/144309 [==============================] - 627s 4ms/step - loss: 1.8169\n",
      "\n",
      "Epoch 00008: loss improved from 1.84139 to 1.81690, saving model to weights-improvement-08-1.8169-bigger.hdf5\n",
      "Epoch 9/50\n",
      "144309/144309 [==============================] - 626s 4ms/step - loss: 1.7471\n",
      "\n",
      "Epoch 00009: loss improved from 1.81690 to 1.74712, saving model to weights-improvement-09-1.7471-bigger.hdf5\n",
      "Epoch 10/50\n",
      "144309/144309 [==============================] - 628s 4ms/step - loss: 1.7095\n",
      "\n",
      "Epoch 00010: loss improved from 1.74712 to 1.70945, saving model to weights-improvement-10-1.7095-bigger.hdf5\n",
      "Epoch 11/50\n",
      "144309/144309 [==============================] - 625s 4ms/step - loss: 1.6864\n",
      "\n",
      "Epoch 00011: loss improved from 1.70945 to 1.68643, saving model to weights-improvement-11-1.6864-bigger.hdf5\n",
      "Epoch 12/50\n",
      "144309/144309 [==============================] - 626s 4ms/step - loss: 1.6471\n",
      "\n",
      "Epoch 00012: loss improved from 1.68643 to 1.64708, saving model to weights-improvement-12-1.6471-bigger.hdf5\n",
      "Epoch 13/50\n",
      "144309/144309 [==============================] - 626s 4ms/step - loss: 1.6164\n",
      "\n",
      "Epoch 00013: loss improved from 1.64708 to 1.61638, saving model to weights-improvement-13-1.6164-bigger.hdf5\n",
      "Epoch 14/50\n",
      "144309/144309 [==============================] - 626s 4ms/step - loss: 1.5875\n",
      "\n",
      "Epoch 00014: loss improved from 1.61638 to 1.58749, saving model to weights-improvement-14-1.5875-bigger.hdf5\n",
      "Epoch 15/50\n",
      "144309/144309 [==============================] - 627s 4ms/step - loss: 1.5598\n",
      "\n",
      "Epoch 00015: loss improved from 1.58749 to 1.55981, saving model to weights-improvement-15-1.5598-bigger.hdf5\n",
      "Epoch 16/50\n",
      "144309/144309 [==============================] - 626s 4ms/step - loss: 1.5378\n",
      "\n",
      "Epoch 00016: loss improved from 1.55981 to 1.53783, saving model to weights-improvement-16-1.5378-bigger.hdf5\n",
      "Epoch 17/50\n",
      "144309/144309 [==============================] - 635s 4ms/step - loss: 1.5174\n",
      "\n",
      "Epoch 00017: loss improved from 1.53783 to 1.51739, saving model to weights-improvement-17-1.5174-bigger.hdf5\n",
      "Epoch 18/50\n",
      "144309/144309 [==============================] - 629s 4ms/step - loss: 1.4963\n",
      "\n",
      "Epoch 00018: loss improved from 1.51739 to 1.49629, saving model to weights-improvement-18-1.4963-bigger.hdf5\n",
      "Epoch 19/50\n",
      "144309/144309 [==============================] - 627s 4ms/step - loss: 1.4787\n",
      "\n",
      "Epoch 00019: loss improved from 1.49629 to 1.47873, saving model to weights-improvement-19-1.4787-bigger.hdf5\n",
      "Epoch 20/50\n",
      "144309/144309 [==============================] - 628s 4ms/step - loss: 1.4636\n",
      "\n",
      "Epoch 00020: loss improved from 1.47873 to 1.46359, saving model to weights-improvement-20-1.4636-bigger.hdf5\n",
      "Epoch 21/50\n",
      "144309/144309 [==============================] - 628s 4ms/step - loss: 1.4430\n",
      "\n",
      "Epoch 00021: loss improved from 1.46359 to 1.44305, saving model to weights-improvement-21-1.4430-bigger.hdf5\n",
      "Epoch 22/50\n",
      "144309/144309 [==============================] - 628s 4ms/step - loss: 1.4321\n",
      "\n",
      "Epoch 00022: loss improved from 1.44305 to 1.43215, saving model to weights-improvement-22-1.4321-bigger.hdf5\n",
      "Epoch 23/50\n",
      "144309/144309 [==============================] - 628s 4ms/step - loss: 1.4158\n",
      "\n",
      "Epoch 00023: loss improved from 1.43215 to 1.41579, saving model to weights-improvement-23-1.4158-bigger.hdf5\n",
      "Epoch 24/50\n",
      "144309/144309 [==============================] - 627s 4ms/step - loss: 1.4026\n",
      "\n",
      "Epoch 00024: loss improved from 1.41579 to 1.40256, saving model to weights-improvement-24-1.4026-bigger.hdf5\n",
      "Epoch 25/50\n",
      "144309/144309 [==============================] - 629s 4ms/step - loss: 1.3937\n",
      "\n",
      "Epoch 00025: loss improved from 1.40256 to 1.39371, saving model to weights-improvement-25-1.3937-bigger.hdf5\n",
      "Epoch 26/50\n",
      "144309/144309 [==============================] - 627s 4ms/step - loss: 1.3851\n",
      "\n",
      "Epoch 00026: loss improved from 1.39371 to 1.38508, saving model to weights-improvement-26-1.3851-bigger.hdf5\n",
      "Epoch 27/50\n",
      "144309/144309 [==============================] - 625s 4ms/step - loss: 1.3694\n",
      "\n",
      "Epoch 00027: loss improved from 1.38508 to 1.36939, saving model to weights-improvement-27-1.3694-bigger.hdf5\n",
      "Epoch 28/50\n",
      "144309/144309 [==============================] - 627s 4ms/step - loss: 1.3613\n",
      "\n",
      "Epoch 00028: loss improved from 1.36939 to 1.36130, saving model to weights-improvement-28-1.3613-bigger.hdf5\n",
      "Epoch 29/50\n",
      "144309/144309 [==============================] - 626s 4ms/step - loss: 1.3531\n",
      "\n",
      "Epoch 00029: loss improved from 1.36130 to 1.35314, saving model to weights-improvement-29-1.3531-bigger.hdf5\n",
      "Epoch 30/50\n",
      "144309/144309 [==============================] - 629s 4ms/step - loss: 1.3429\n",
      "\n",
      "Epoch 00030: loss improved from 1.35314 to 1.34291, saving model to weights-improvement-30-1.3429-bigger.hdf5\n",
      "Epoch 31/50\n",
      "144309/144309 [==============================] - 629s 4ms/step - loss: 1.3375\n",
      "\n",
      "Epoch 00031: loss improved from 1.34291 to 1.33748, saving model to weights-improvement-31-1.3375-bigger.hdf5\n",
      "Epoch 32/50\n",
      "144309/144309 [==============================] - 629s 4ms/step - loss: 1.3303\n",
      "\n",
      "Epoch 00032: loss improved from 1.33748 to 1.33027, saving model to weights-improvement-32-1.3303-bigger.hdf5\n",
      "Epoch 33/50\n",
      "144309/144309 [==============================] - 637s 4ms/step - loss: 1.3229\n",
      "\n",
      "Epoch 00033: loss improved from 1.33027 to 1.32292, saving model to weights-improvement-33-1.3229-bigger.hdf5\n",
      "Epoch 34/50\n",
      "144309/144309 [==============================] - 642s 4ms/step - loss: 1.3219\n",
      "\n",
      "Epoch 00034: loss improved from 1.32292 to 1.32194, saving model to weights-improvement-34-1.3219-bigger.hdf5\n",
      "Epoch 35/50\n",
      "144309/144309 [==============================] - 631s 4ms/step - loss: 1.3130\n",
      "\n",
      "Epoch 00035: loss improved from 1.32194 to 1.31297, saving model to weights-improvement-35-1.3130-bigger.hdf5\n",
      "Epoch 36/50\n",
      "144309/144309 [==============================] - 638s 4ms/step - loss: 1.3048\n",
      "\n",
      "Epoch 00036: loss improved from 1.31297 to 1.30484, saving model to weights-improvement-36-1.3048-bigger.hdf5\n",
      "Epoch 37/50\n",
      "144309/144309 [==============================] - 637s 4ms/step - loss: 1.2942\n",
      "\n",
      "Epoch 00037: loss improved from 1.30484 to 1.29423, saving model to weights-improvement-37-1.2942-bigger.hdf5\n",
      "Epoch 38/50\n",
      "144309/144309 [==============================] - 651s 5ms/step - loss: 1.2940\n",
      "\n",
      "Epoch 00038: loss improved from 1.29423 to 1.29404, saving model to weights-improvement-38-1.2940-bigger.hdf5\n",
      "Epoch 39/50\n",
      "144309/144309 [==============================] - 649s 4ms/step - loss: 1.2890\n",
      "\n",
      "Epoch 00039: loss improved from 1.29404 to 1.28904, saving model to weights-improvement-39-1.2890-bigger.hdf5\n",
      "Epoch 40/50\n",
      "144309/144309 [==============================] - 644s 4ms/step - loss: 1.2841\n",
      "\n",
      "Epoch 00040: loss improved from 1.28904 to 1.28407, saving model to weights-improvement-40-1.2841-bigger.hdf5\n",
      "Epoch 41/50\n",
      "144309/144309 [==============================] - 643s 4ms/step - loss: 1.2778\n",
      "\n",
      "Epoch 00041: loss improved from 1.28407 to 1.27777, saving model to weights-improvement-41-1.2778-bigger.hdf5\n",
      "Epoch 42/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144309/144309 [==============================] - 628s 4ms/step - loss: 1.2787\n",
      "\n",
      "Epoch 00042: loss did not improve from 1.27777\n",
      "Epoch 43/50\n",
      "144309/144309 [==============================] - 628s 4ms/step - loss: 1.2744\n",
      "\n",
      "Epoch 00043: loss improved from 1.27777 to 1.27443, saving model to weights-improvement-43-1.2744-bigger.hdf5\n",
      "Epoch 44/50\n",
      "144309/144309 [==============================] - 628s 4ms/step - loss: 1.2726\n",
      "\n",
      "Epoch 00044: loss improved from 1.27443 to 1.27257, saving model to weights-improvement-44-1.2726-bigger.hdf5\n",
      "Epoch 45/50\n",
      "144309/144309 [==============================] - 627s 4ms/step - loss: 1.2699\n",
      "\n",
      "Epoch 00045: loss improved from 1.27257 to 1.26987, saving model to weights-improvement-45-1.2699-bigger.hdf5\n",
      "Epoch 46/50\n",
      "144309/144309 [==============================] - 628s 4ms/step - loss: 1.2665\n",
      "\n",
      "Epoch 00046: loss improved from 1.26987 to 1.26647, saving model to weights-improvement-46-1.2665-bigger.hdf5\n",
      "Epoch 47/50\n",
      "144309/144309 [==============================] - 627s 4ms/step - loss: 1.2659\n",
      "\n",
      "Epoch 00047: loss improved from 1.26647 to 1.26586, saving model to weights-improvement-47-1.2659-bigger.hdf5\n",
      "Epoch 48/50\n",
      "144309/144309 [==============================] - 628s 4ms/step - loss: 1.2604\n",
      "\n",
      "Epoch 00048: loss improved from 1.26586 to 1.26044, saving model to weights-improvement-48-1.2604-bigger.hdf5\n",
      "Epoch 49/50\n",
      "144309/144309 [==============================] - 628s 4ms/step - loss: 1.2591\n",
      "\n",
      "Epoch 00049: loss improved from 1.26044 to 1.25906, saving model to weights-improvement-49-1.2591-bigger.hdf5\n",
      "Epoch 50/50\n",
      "144309/144309 [==============================] - 629s 4ms/step - loss: 1.2591\n",
      "\n",
      "Epoch 00050: loss did not improve from 1.25906\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28bdfa7c208>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(LSTM(256))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Dense(y.shape[1], activation='softmax'))\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "# fit the model\n",
    "model2.fit(X, y, epochs=50, batch_size=64, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\"  queen, who had meanwhile been examining the roses.\n",
      "'off with their heads!' and the procession moved \"\n",
      " on she was so tale the rame as the way that she was salking in her face, and she was so lane out that she was sealiy and stickly and stre the shmught that she was now the sight size to sist the way that she was seady to sake the look of the thoe, and the thing said to herself, and the three gardeners the sabbit aw the same thing so hear the way that she was now the window, and the thing said that she was now the right size to sist the way that she was salking in her face, and she was so lane out that she was sealiy and stickly and stre the shmught that she was now the sight size to sist the way that she was seady to sake the look of the thoe, and the thing said to herself, and the three gardeners the sabbit aw the same thing so hear the way that she was now the window, and the thing said that she was now the right size to sist the way that she was salking in her face, and she was so lane out that she was sealiy and stickly and stre the shmught that she was now the sight size to sist t\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# load the network weights\n",
    "filename = \"weights-improvement-49-1.2591-bigger.hdf5\"\n",
    "model2.load_weights(filename)\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print (\"Seed:\")\n",
    "print (\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "\tx = x / float(n_vocab)\n",
    "\tprediction = model2.predict(x, verbose=0)\n",
    "\tindex = numpy.argmax(prediction)\n",
    "\tresult = int_to_char[index]\n",
    "\tseq_in = [int_to_char[value] for value in pattern]\n",
    "\tsys.stdout.write(result)\n",
    "\tpattern.append(index)\n",
    "\tpattern = pattern[1:len(pattern)]\n",
    "print (\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seed:\n",
    "\"  of expecting nothing but out-of-the-way\n",
    "things to happen, that it seemed quite dull and stupid for  \"\n",
    "a little before she had not going on stettion, and then a little shriek so the trial's ang rried to see if she was this time the way that she was bbout in the lock turtle and alice said to the karter with the door, she was now the soog and alice so get out of the wood, 'i've sried the baby?'\n",
    "\n",
    "'well, i've tried to be a git head beautifyl toop-' said alice. \n",
    "'well, i mear the duece,' said the duchess, 'and they live the same thing is to be a good deal the things aeautifu oreer to some tomething is the way to say it a long way in the soo of the soo of her head to her to tp the way that she was now the right size, but the three gardeners the sabbit and the lock turtle sabbit all the tame thing about ier face on the wood, 'i've sried the baby?'\n",
    "\n",
    "'well, i've tried to be a linttes ' said the mock turtle.\n",
    "\n",
    "'what a pight to be a dancy ' said the mock turtle. \n",
    "'what said the sea,' the karge raid: 'it said in the soog of the shate and the becense of the soog, \n",
    "the dormouse said to the kury ae afa\n",
    "Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
